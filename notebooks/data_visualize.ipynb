{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)\n",
    "\n",
    "This notebook is used to perform EDA on raw data or on features generated from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "import dtale\n",
    "import pandas as pd\n",
    "from zenml import pipeline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from configs import configs\n",
    "from configs.parser import ConfigParser\n",
    "from data_manager.loaders import StructuredData\n",
    "from steps import (\n",
    "    data_formatter,\n",
    "    data_loader,\n",
    "    features_engineer_creator,\n",
    "    features_generator,\n",
    ")\n",
    "\n",
    "data_loader = data_loader.with_options(enable_cache=True)\n",
    "data_formatter = data_formatter.with_options(enable_cache=True)\n",
    "features_engineer_creator = features_engineer_creator.with_options(enable_cache=True)\n",
    "features_generator = features_generator.with_options(enable_cache=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline(enable_cache=True)\n",
    "def load_data() -> StructuredData:\n",
    "    cfg_parser = ConfigParser()\n",
    "    data = data_loader(cfg_parser.general().without_varieties(), cfg_parser.multispectral())\n",
    "    data = data_formatter(data, cfg_parser.general(), cfg_parser.formatter())\n",
    "    features_engineer = features_engineer_creator(data, cfg_parser.features())\n",
    "    data, _, _ = features_generator(features_engineer, data)\n",
    "    return data\n",
    "\n",
    "# Set the TOML config file as an environment variable (parsed in the pipelines)\n",
    "os.environ[configs.TOML_ENV_NAME] = str(configs.TOML_DIR / \"clf/umap_varieties.toml\")\n",
    "# Run the pipeline only the first time to load the data\n",
    "load_data()\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run = load_data.model.last_successful_run\n",
    "data = last_run.steps[\"features_generator\"]\n",
    "data = data.outputs[\"data_train_feat\"].load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge two dataframes\n",
    "df = pd.concat([data.data, data.meta], axis=1)\n",
    "# Assigning a reference to a running D-Tale process.\n",
    "d = dtale.show(df)\n",
    "# Using Python's `webbrowser` package it will try and open your server's default browser to this process.\n",
    "d.open_browser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutting down D-Tale process\n",
    "# d.kill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0,'..')\n",
    "\n",
    "import pygwalker as pyg\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "from configs import paths, configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_label=\"LICOR_leaf\"\n",
    "excel_file = paths.PATHS_MEASUREMENTS[regression_label][1]\n",
    "print(excel_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(excel_file)\n",
    "walker = pyg.walk(\n",
    "    df,\n",
    "    spec=str(configs.SAVE_DIR / \"walker_spec.json\"),    # this json file will save your chart state, you need to click save button in ui mannual when you finish a chart, 'autosave' will be supported in the future.\n",
    "    use_kernel_calc=True,          # set `use_kernel_calc=True`, pygwalker will use duckdb as computing engine, it support you explore bigger dataset(<=100GB).\n",
    "    dark=\"light\",\n",
    "    store_chart_data=True,          \n",
    "    hideDataSourceConfig=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
