{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate results\n",
    "\n",
    "This notebook is used to explore disease (Alternaria) present on the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from configs import configs\n",
    "from database.db import SQLiteDatabase\n",
    "from utils.metrics import calculate_classification_metrics\n",
    "from utils.plot_utils import save_plot_figure\n",
    "from utils.utils import ensure_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"alternaria_clf\"\n",
    "model_name = \"alternaria_b_clf\"\n",
    "\n",
    "db = SQLiteDatabase()\n",
    "records = db.get_records(is_latest=True, model_name=model_name)\n",
    "container = {}\n",
    "for record in records:\n",
    "\tfor data, predictions in zip(record.data, record.predictions):\n",
    "\t\tdata_name = data.name\n",
    "\t\tdata_content = data.content\n",
    "\t\tpred_content = predictions.content\n",
    "\t\tcontainer[data_name] = (data_content, pred_content)\n",
    "\n",
    "mapping = {'class 1': 'Healthy', 'class 2': 'Infected'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countplot(data_name: str):\n",
    "\twith save_plot_figure(save_path=ensure_dir(configs.SAVE_RESULTS_DIR / model_name) / f\"{data_name}_distribution.pdf\", \n",
    "                          figsize=(8,3)) as (fig, ax):\n",
    "\t\tif data_name == \"all\":\n",
    "\t\t\tdata_names = [\"train\", \"test\"]\n",
    "\n",
    "\t\tif isinstance(data_name, str) and not data_name == \"all\":\n",
    "\t\t\tdata_names = [data_name]\n",
    "   \n",
    "\t\tdfs = []\n",
    "\t\tfor data_name in data_names:\n",
    "\t\t\tmeta = container[data_name][0].meta.reset_index(drop=True)\n",
    "\t\t\ttarget = container[data_name][0].target\n",
    "\t\t\tdf = pd.concat((meta, pd.DataFrame(target.label)), axis=1)\n",
    "\t\t\tdf[\"label\"] = df[\"label\"].map(mapping)\n",
    "\t\t\tdfs.append(df)\n",
    "\n",
    "\t\tdf = pd.concat(dfs, axis=0)\n",
    "\n",
    "\t\tsns.countplot(data=df, x=\"varieties\", hue=\"label\", palette=[\"darkgoldenrod\", \"forestgreen\"], alpha=0.5)\n",
    "\t\tax.legend(framealpha=0)\n",
    "\n",
    "\t\tplt.xlabel('Variety')\n",
    "\t\tplt.ylabel('Count')\n",
    "\t\tplt.title(data_name)\n",
    "\t\tax.spines[\"right\"].set_linewidth(0)\n",
    "\t\tax.spines[\"top\"].set_linewidth(0)\n",
    "\n",
    "countplot(\"train\")\n",
    "countplot(\"test\")\n",
    "countplot(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_name = \"test\"\n",
    "meta = container[data_name][0].meta.reset_index(drop=True)\n",
    "target = container[data_name][0].target\n",
    "\n",
    "y_true = target.value.to_numpy()\n",
    "y_pred = container[data_name][1].predictions\n",
    "\n",
    "df = pd.DataFrame.from_dict(\n",
    "\t{\n",
    "\t\t\"varieties\": meta[\"varieties\"],\n",
    "\t\t\"y_true\": y_true,\n",
    "\t\t\"y_pred\": y_pred,\n",
    "\t}\n",
    ")\n",
    "\n",
    "average = \"weighted\"\n",
    "metrics = {}\n",
    "metrics_by_class = {}\n",
    "for name, group in df.groupby(\"varieties\"):\n",
    "\tmetrics_by_class[name] = calculate_classification_metrics(group.y_true, group.y_pred, average=None)\n",
    "\tmetrics[name] = calculate_classification_metrics(group.y_true, group.y_pred, average=average)\n",
    "\n",
    "\n",
    "print(f\"Metrics on {data_name} data:\")\n",
    "print(\"\\n--> Metrics per treatment:\")\n",
    "for variety, metric in metrics_by_class.items():\n",
    "\tprint(variety)\n",
    "\tfor idx, (f1, precision, recall) in enumerate(zip(metric.f1, metric.precision, metric.recall)):\n",
    "\t\ttreatment = mapping[container[\"test\"][0].target.encoding.to_dict()[idx]]\n",
    "\t\tprint(f\"   {treatment:<9} -> F1: {f1:.2f} | Precision: {precision:.2f} | Recall: {recall:.2f}\" )\n",
    "\n",
    "print(\"\\n--> Metrics per variety:\")\n",
    "print(\"Variety    | Accuracy |  F1  | Precision | Recall\")\n",
    "for variety, metric in metrics.items():\n",
    "    print(\n",
    "        f\"{variety:<10} |     {metric.accuracy:.2f} | {metric.f1:.2f} \"\n",
    "        f\"| {metric.precision:.2f}      | {metric.recall:.2f}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n--> Average metrics on:\\n{calculate_classification_metrics(y_true=y_true, y_pred=y_pred, average=average)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
